# -*- coding: utf-8 -*-
"""using-UCI_database-v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A8_vGJxR8SZ9XPq2gk2vOBBi4FQPUFet
"""



"""**Comparison of clustering algorithms:**___
By **Anwar Hashem** PhD student.
##Clustering with dataset from the UCI database

### Importing Libraries
"""

# Import Libraries
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
## from sklearn.datasets.samples_generator import make_blobs
from sklearn.datasets import make_blobs, make_moons
from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN
from sklearn import metrics
from sklearn.metrics import pairwise_distances_argmin
from sklearn.metrics import silhouette_samples, silhouette_score
import time
from sklearn.cluster import AgglomerativeClustering, SpectralClustering, Birch, MeanShift, AffinityPropagation
#import arff, numpy as np
from scipy.io import arff
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_transformer

#from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics.cluster import homogeneity_score

from sklearn.neighbors import NearestNeighbors
import scipy.cluster.hierarchy as sch
import zipfile

# I select dataset (Dry Bean, Donated on 9/13/2020) from https://archive.ics.uci.edu/dataset/602/dry+bean+dataset
'''
Dry Beans: Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera.
A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.
'''

with zipfile.ZipFile('/content/dry+bean+dataset.zip', 'r') as myzip:
  myzip.extractall('./')

file_path = "/content/DryBeanDataset/Dry_Bean_Dataset.xlsx"
file_path

file_path='/content/Dry_Bean_Dataset.xlsx'
data_r = pd.read_excel(file_path)

data_r.shape # 45211,17
data_r.head()

Ytr=data_r["Class"]
Xtr=data_r.drop(["Class"],axis=1)


Xtr_r = data_r.iloc[:, :-1].values
ytr_r = data_r.iloc[:, -1].values

Xtr

# To describe the fields types and ensure that non-values are exists and all input data contains numeric values
data_r.info()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Enc_Ytr = le.fit_transform(Ytr)
Enc_Ytr

Ytr.value_counts()

Enc_Ytr=pd.DataFrame(Enc_Ytr)
Enc_Ytr.value_counts()

Xtr.info()

from sklearn.tree import DecisionTreeRegressor
Tree_model_r= DecisionTreeRegressor(random_state=0, max_depth=5)
Tree_model_r.fit(Xtr_r, Enc_Ytr)

col_name_r=list(Xtr.columns.values)
col_name_r

#feature importance
F_import_r=Tree_model_r.feature_importances_*100
#F_import

for i in range(Xtr.shape[1]):
  print(col_name_r[i],F_import_r[i])

#So, This is more Feauters Impotant (MinorAxisLength + Perimeter ) so we will plot MinorAxisLength and Perimeter

pca=PCA(n_components=2)
X_principal= pca.fit_transform(Xtr)

X_principal

#be delete (only for test)
#data_r = sns.load_dataset(data_r_path)
sns.set_style("darkgrid")

sns.scatterplot(x='MinorAxisLength' , y='Perimeter', data=data_r,hue='Class')
plt.title("MinorAxisLength vs. Perimeter")
plt.xlabel('Perimeter')


plt.show()

#data_r = sns.load_dataset(Xtr_r)
sns.set_style("darkgrid")
sns.scatterplot(x='MinorAxisLength' , y='Perimeter', data=Xtr)
plt.title("MinorAxisLength vs. Perimeter")
#plt.xlabel('Perimeter')
plt.show()

# silhouette score
score_r = []
print('init= Cluster=silhouette_score')
n=20
for i in range(2,n):
  #km_r= KMeans(init='k-means++', n_clusters=i, n_init=10, random_state=0)
  km_r= KMeans(n_clusters=i)

  km_r.fit(Xtr)
  result = km_r.labels_
  #print(i , '    '  , silhouette_score(Xtr , result))
  score_r.append(silhouette_score(Xtr , result))

plt.plot(range(2,n) , score_r)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette score')
plt.show()

# By first looking at the data plotو  we find that the appropriate value for the number of clusters is: 5
n_clusters_r=5
Xtr=StandardScaler().fit_transform(Xtr)

km_r= KMeans(n_clusters=n_clusters_r)
y_kmeans= km_r.fit_predict(Xtr)


#visualising the cluster
#Xtr_r[km_r_labels == 0, 0]
#Xtr_r[km_r_labels == 0, 1]
plt.scatter(Xtr[y_kmeans == 0, 0], Xtr[y_kmeans == 0, 1], s = 10, c = 'r')
plt.scatter(Xtr[y_kmeans == 1, 0], Xtr[y_kmeans == 1, 1], s = 10, c = 'b')
plt.scatter(Xtr[y_kmeans == 2, 0], Xtr[y_kmeans == 2, 1], s = 10, c = 'g')
plt.scatter(Xtr[y_kmeans == 3, 0], Xtr[y_kmeans == 3, 1], s = 10, c = 'y')
plt.scatter(Xtr[y_kmeans == 4, 0], Xtr[y_kmeans == 4, 1], s = 10, c = 'm')
#plt.scatter(Xtr[y_kmeans == 5, 0], Xtr[y_kmeans == 5, 1], s = 10, c = 'c')
#plt.scatter(Xtr[y_kmeans == 6, 0], Xtr[y_kmeans == 6, 1], s = 10, c = 'k')
#plt.scatter(Xtr_r[y_kmeans == 7, 0], Xtr_r[y_kmeans == 7, 1], s = 10, c = 'w')
plt.scatter(km_r.cluster_centers_[:, 0], km_r.cluster_centers_[:, 1], s = 100, c = '#ff7f0e')
# #1f77b4'

plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
#plt.legend()
plt.show()

# ##################################################################
# Compute DBSCAN

#Xtr=StandardScaler().fit_transform(Xtr)

#for kk in range(1,10): # min_samples=5 gives us good result, we fixed min_samples=5

#for mm in (0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1,1.1,1.2,1.3,1.4,1.5,1.6,1.7,1.8,1.9,2):
  # eps=1.1,1.2 gives us good result(cluster 7,5 and noise=497,379), we fixed eps=1.2

  #DBSCANModel = DBSCAN(metric='euclidean',eps=0.3,min_samples=kk,algorithm='kd_tree')#it can be ball_tree, kd_tree, brute
  #DBSCANModel = DBSCAN(metric='euclidean',eps=0.3,min_samples=kk,algorithm='brute')#it can be ball_tree, kd_tree, brute
  #DBSCANModel = DBSCAN(metric='euclidean',eps=mm,min_samples=5,algorithm='auto')#it can be ball_tree, kd_tree, brute
DBSCANModel = DBSCAN(metric='euclidean',eps=1.2,min_samples=5,algorithm='auto')#it can be ball_tree, kd_tree, brute

  #DBSCANModel = DBSCAN(eps=0.3,min_samples=10)
y_pred_train = DBSCANModel.fit_predict(Xtr)
  #DBSCANModel.labels_
  #DBSCANModel = DBSCAN(eps=0.8, min_samples=10).fit(Xtr_r)
core_samples_mask = np.zeros_like(DBSCANModel.labels_, dtype=bool)
core_samples_mask[DBSCANModel.core_sample_indices_] = True
labels = DBSCANModel.labels_
labels
  #print('\n min_samples=',kk)
#print('\n eps=',mm)
print(pd.DataFrame(labels).value_counts())
  # Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)
print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)

# Commented out IPython magic to ensure Python compatibility.
# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
'''
print("Homogeneity: %0.3f" % metrics.homogeneity_score(Xtr, DBSCANModel.labels_))
#print("%.6f" % homogeneity_score(Xtr, y_pred_train))

print("Completeness: %0.3f" % metrics.completeness_score(Xtr, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(Xtr, labels))
print("Adjusted Rand Index: %0.3f"
#       % metrics.adjusted_rand_score(Xtr, labels))
#labels_true must be 1D: shape is (13611, 16)

print("Adjusted Mutual Information: %0.3f"
#       % metrics.adjusted_mutual_info_score(Enc_Ytr, y_pred_train,
                                           average_method='arithmetic'))

print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(Xtr, y_pred_train))
'''

#From the figure above, we find that the appropriate value for the number of clusters is: 5


# Perform K-Means clustering
#km_s = KMeans(n_clusters=7)
#km_r= KMeans(init='k-means++', n_clusters=n_clusters_r, n_init=10, random_state=0)
km_r= KMeans(n_clusters=n_clusters_r)

km_r_labels = km_r.fit_predict(Xtr)

# Plot the data points and cluster assignments for K-Means
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 2)
#plt.scatter(Xtr_r[:, 0], Xtr_r[:, 1], c=km_r_labels, cmap='viridis', edgecolors='k', alpha=0.7)
plt.scatter(Xtr[km_r_labels == 0, 0], Xtr[km_r_labels == 0, 1], s = 10, c = 'r')
plt.scatter(Xtr[km_r_labels == 1, 0], Xtr[km_r_labels == 1, 1], s = 10, c = 'b')
plt.scatter(Xtr[km_r_labels == 2, 0], Xtr[km_r_labels == 2, 1], s = 10, c = 'g')
plt.scatter(Xtr[km_r_labels == 3, 0], Xtr[km_r_labels == 3, 1], s = 10, c = 'y')
plt.scatter(Xtr[km_r_labels == 4, 0], Xtr[km_r_labels == 4, 1], s = 10, c = 'm')
plt.scatter(Xtr[km_r_labels == 5, 0], Xtr[km_r_labels == 5, 1], s = 10, c = 'c')
plt.scatter(Xtr[km_r_labels == 6, 0], Xtr[km_r_labels == 6, 1], s = 10, c = 'k')
#plt.scatter(Xtr_r[km_r_labels == 7, 0], Xtr_r[y_kmeans == 7, 1], s = 10, c = 'w')
plt.scatter(km_r.cluster_centers_[:, 0], km_r.cluster_centers_[:, 1], s = 100, c = '#ff7f0e')
# #1f77b4'

plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')


# Perform DBSCAN clustering
dbscan_r = DBSCAN(eps=1.1, min_samples=5)
dbscan_r_labels = dbscan_r.fit_predict(Xtr)

# Plot the data points and cluster assignments for DBSCAN


plt.subplot(1, 2, 1)
#plt.scatter(Xtr[:, 0], Xtr[:, 1], c=dbscan_r_labels, cmap='viridis', edgecolors='k', alpha=0.7)

plt.scatter(Xtr[dbscan_r_labels == 0, 0], Xtr[dbscan_r_labels == 0, 1], s = 6, c = 'r')
plt.scatter(Xtr[dbscan_r_labels == 1, 0], Xtr[dbscan_r_labels == 1, 1], s = 6, c = 'b')
plt.scatter(Xtr[dbscan_r_labels == 2, 0], Xtr[dbscan_r_labels == 2, 1], s = 6, c = 'g')
plt.scatter(Xtr[dbscan_r_labels == 3, 0], Xtr[dbscan_r_labels == 3, 1], s = 6, c = 'y')
plt.scatter(Xtr[dbscan_r_labels == 4, 0], Xtr[dbscan_r_labels == 4, 1], s = 6, c = 'm')
#plt.scatter(Xtr[dbscan_r_labels == 5, 0], Xtr[dbscan_r_labels == 5, 1], s = 6, c = 'c')
#plt.scatter(Xtr[dbscan_r_labels == 6, 0], Xtr[dbscan_r_labels == 6, 1], s = 6, c = 'k')
#plt.scatter(dbscan_r_labels[:, 0], dbscan_r_labels[:, 1], s = 100, c = '#ff7f0e')


plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')


plt.tight_layout()
plt.show()

aa=pd.DataFrame(dbscan_r_labels)
aa.value_counts()

# Define clustering algorithms

km_r=KMeans(n_clusters=n_clusters_r)
AgglomerativeClustering_r=AgglomerativeClustering(n_clusters=n_clusters_r)
DBSCAN_r=DBSCAN(eps=1.1, min_samples=5)
SpectralClustering_r=SpectralClustering(n_clusters=n_clusters_r)
Birch_r=Birch(n_clusters=n_clusters_r)
MiniBatchKMeans_r=MiniBatchKMeans(n_clusters=n_clusters_r)
MeanShift_r=MeanShift()
AffinityPropagation_r =AffinityPropagation()

Models=[km_r,AgglomerativeClustering_r,DBSCAN_r,SpectralClustering_r,Birch_r,MiniBatchKMeans_r,MeanShift_r,AffinityPropagation_r]

# Compare clustering algorithms
fig, axes = plt.subplots(2, 4, figsize=(15, 8))
fig.suptitle('Comparison of Clustering Algorithms')

ModelsScore = {}
# for all models above
for Model in Models:
  labels = Model.fit_predict(Xtr)
  # Plot the results
  plt.scatter(Xtr[:, 0], Xtr[:, 1], c=labels, cmap='viridis', edgecolors='k')
  plt.title(Model)
  #plt.flatten.axis('off')
  plt.show()

# Define clustering algorithms

km_r=KMeans(n_clusters=n_clusters_r)
AgglomerativeClustering_r=AgglomerativeClustering(n_clusters=n_clusters_r)
DBSCAN_r=DBSCAN(eps=1.1, min_samples=5)
SpectralClustering_r=SpectralClustering(n_clusters=n_clusters_r)
Birch_r=Birch(n_clusters=n_clusters_r)
MiniBatchKMeans_r=MiniBatchKMeans(n_clusters=n_clusters_r)
MeanShift_r=MeanShift()
AffinityPropagation_r =AffinityPropagation()

Models=[km_r,AgglomerativeClustering_r,DBSCAN_r,SpectralClustering_r,Birch_r,MiniBatchKMeans_r,MeanShift_r,AffinityPropagation_r]

# Compare clustering algorithms
ModelsScore = {}
score_r = []
# for all models above
print('silhouette_score Of Models')
for Model in Models:
  labels = Model.fit_predict(Xtr)

  result = Model.labels_

  print(Model , '    '  , silhouette_score(Xtr , result))
  score_r.append(silhouette_score(Xtr , result))

"""The above results show that
- DBSCAN algorithm is more accurate(silhouette_score=10.7), but it deleted several data and considered it an outlier. Next comes AffinityPropagation (silhouette_score=13).
- SpectralClustering and MeanShift are less accurate (silhouette_score=49).
- The results of KMeans,Birch,MiniBatchKMeans,AgglomerativeClustering
are close (silhouette_score=30)
"""

#----------------------------------------------------

#Applying PCAModel Model

'''
sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False,svd_solver='auto’, tol=0.0,
                          iterated_power='auto’, random_state=None)
'''

PCAModel = PCA(n_components=2, svd_solver='auto')#it can be full,arpack,randomized
PCAModel.fit(Xtr)

#Calculating Details
print('PCAModel Train Score is : ' , PCAModel.score(Xtr))
print('PCAModel No. of components is : ' , PCAModel.components_)
print('PCAModel Explained Variance is : ' , PCAModel.explained_variance_)
print('PCAModel Explained Variance ratio is : ' , PCAModel.explained_variance_ratio_)
print('PCAModel singular value is : ' , PCAModel.singular_values_)
print('PCAModel mean is : ' , PCAModel.mean_)
print('PCAModel noise variance is : ' , PCAModel.noise_variance_)

#----------------------------------------------------

#Applying AggClusteringModel Model

'''
sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean’, memory=None, connectivity=None,
                                        compute_full_tree='auto’, linkage=’ward’,pooling_func=’deprecated’)
'''

AggClusteringModel = AgglomerativeClustering(n_clusters=5,affinity='euclidean',# it can be l1,l2,manhattan,cosine,precomputed
                                             linkage='ward')# it can be complete,average,single

y_pred_train = AggClusteringModel.fit_predict(Xtr)

#draw the Hierarchical graph for Training set
dendrogram = sch.dendrogram(sch.linkage(Xtr[: 30,:], method = 'ward'))# it can be complete,average,single
plt.title('Training Set')
plt.xlabel('X Values')
plt.ylabel('Distances')
plt.show()


#draw the Scatter for Train set
plt.scatter(Xtr[y_pred_train == 0, 0], Xtr[y_pred_train == 0, 1], s = 10, c = 'red', label = 'Cluster 1')
plt.scatter(Xtr[y_pred_train == 1, 0], Xtr[y_pred_train == 1, 1], s = 10, c = 'blue', label = 'Cluster 2')
plt.scatter(Xtr[y_pred_train == 2, 0], Xtr[y_pred_train == 2, 1], s = 10, c = 'green', label = 'Cluster 3')
plt.scatter(Xtr[y_pred_train == 3, 0], Xtr[y_pred_train == 3, 1], s = 10, c = 'cyan', label = 'Cluster 4')
plt.scatter(Xtr[y_pred_train == 4, 0], Xtr[y_pred_train == 4, 1], s = 10, c = 'magenta', label = 'Cluster 5')
plt.title('Training Set')
plt.xlabel('X Value')
plt.ylabel('y Value')
plt.legend()
plt.show()

plt.show()

"""#**Conclusion:**
#1- DBSCAN is most efficient among other algorithms in determining the accuracy of the number of clusters.
# 2- The key advantage of DBSCAN is its ability to identify clusters of arbitrary shapes and handle noise effectively.
# 3- After applying PCA's algorithm, the algorithms gave better accuracy.
# 4- There is almost no difference between KMeans algorithm and MiniBatchKMeans algorithm in terms of results, except that MiniBatchKMeans algorithm is slightly faster.


#So, in most cases we can use several algorithms to reach the most efficient model, as follows:
# - PCA Algorithm for Reduction for data.
# - DBSCAN for Remove Outlier and preprocessing
# - KMeans for Clustering
# - Logistic for Classification
# - Logistic Regression fro prediction

"""