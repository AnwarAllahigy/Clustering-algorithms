# -*- coding: utf-8 -*-
"""using-synthetic_data_v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZJOYZBW4RdBdOuKCElSg92Vlv5vD1Cyd
"""



"""## **Comparison of clustering algorithms:**
## By **Anwar Hashem** (PhD Student)
## Clustering with  a synthetic dataset
"""

import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
## from sklearn.datasets.samples_generator import make_blobs
from sklearn.datasets import make_blobs, make_moons
from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN
from sklearn.metrics import pairwise_distances_argmin
from sklearn import metrics
from sklearn.metrics import silhouette_samples, silhouette_score
import time
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering, SpectralClustering, Birch, MeanShift, AffinityPropagation

# ########################################################################
# Generate a synthetic data
## temp np.random.seed(0)


centers_s =[[1, 1], [-1, -1], [1, -1]]
#n_clusters_s = len(centers_s)
Xtr, labels_true = make_blobs(n_samples=750, centers=centers_s, cluster_std=0.4, random_state=0)

Xtr_s = StandardScaler().fit_transform(Xtr)

# buali Xtr1, labels_true = make_blobs(n_samples=300, centers=4, random_state=42)
## temp Xtr_s, labels_true = make_blobs(n_samples=3000, centers=7, cluster_std=0.7)

# Generate synthetic data with two crescent moons
#Xtr_s, _ = make_moons(n_samples=2000, noise=0.05, random_state=42)
## centers_s = [[1, 1], [-1, -1], [1, -1]]
## Xtr_s, _ = make_moons(n_samples=2000, noise=0.05, random_state=0)

Xtr_s.shape
labels_true

Xtr_s

#see it using a scatter plot:
#plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 10, c = 'r')
#plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 10, c = 'b')
#plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 10, c = 'g')


plt.scatter(Xtr_s[:,0],Xtr_s[:,1] ,c ='g' , s = 8)
#plt.scatter(Xtr_s[:,0],Xtr_s[:,1] ,c ='b' , s = 10)


#for j in range(len(centers)):
 #   plt.scatter(centers[j,0],centers[j,1] ,c ='r' , s = 100)
plt.title('a synthetic dataset')
plt.xlabel('Feauture 1')
plt.ylabel('Feauture 2')
plt.show()


#plt.scatter(kmean.cluster_centers_[:, 0], kmean.cluster_centers_[:, 1], s = 100, c = 'y')
#plt.show()

# By first looking at the data plotÙˆ  we find that the appropriate value for the number of clusters is:
n_clusters_s=6


#To be more sure, we will aplay the Elbow method
# The Elbow Method
inertia_val_s=[]
print('Cluster=Inertia')
n=20
for i in range(1,n):
    #km_s = KMeans(n_clusters= n )
    km_s= KMeans(init='k-means++', n_clusters=i, n_init=10, random_state=0)
    km_s.fit(Xtr_s)
    inertia_val_s.append(km_s.inertia_)
    print(i , '    '  , km_s.inertia_)


plt.plot(range(1,n) , inertia_val_s)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
plt.show()

score_s = []
print('init= Cluster=silhouette_score')
n=20
for i in range(2,n):
  km_s = KMeans(n_clusters= i )
  #km_s= KMeans(init='random', n_clusters=i, n_init=10, random_state=0)
  #km_s= KMeans(init='k-means++', n_clusters=i, n_init=10, random_state=0)

  km_s.fit(Xtr_s)
  result = km_s.labels_
  print(i , '    '  , silhouette_score(Xtr_s , result))
  score_s.append(silhouette_score(Xtr_s , result))

plt.plot(range(2,n) , score_s)
plt.show()

#From the figure above, we find that the appropriate value for the number of clusters is: 6
n_clusters_s=6

# Perform K-Means clustering
#km_s = KMeans(n_clusters=7)
km_s= KMeans(init='k-means++', n_clusters=n_clusters_s, n_init=10, random_state=0)
km_s_labels = km_s.fit_predict(Xtr_s)

# Plot the data points and cluster assignments for K-Means
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 2)
plt.scatter(Xtr_s[:, 0], Xtr_s[:, 1], c=km_s_labels, cmap='viridis', edgecolors='k', alpha=0.7)
plt.title('K-Means Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')


# Perform DBSCAN clustering
dbscan_s = DBSCAN(eps=0.3, min_samples=10)
dbscan_s_labels = dbscan_s.fit_predict(Xtr_s)

# Plot the data points and cluster assignments for DBSCAN


plt.subplot(1, 2, 1)
plt.scatter(Xtr_s[:, 0], Xtr_s[:, 1], c=dbscan_s_labels, cmap='viridis', edgecolors='k', alpha=0.7)
plt.title('DBSCAN Clustering')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')


plt.tight_layout()
plt.show()

# ##################################################################
# Compute DBSCAN
db = DBSCAN(eps=0.3, min_samples=10).fit(Xtr_s)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_
labels

# Commented out IPython magic to ensure Python compatibility.
# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
n_noise_ = list(labels).count(-1)

print('Estimated number of clusters: %d' % n_clusters_)
print('Estimated number of noise points: %d' % n_noise_)
print("Homogeneity: %0.3f" % metrics.homogeneity_score(labels_true, labels))
print("Completeness: %0.3f" % metrics.completeness_score(labels_true, labels))
print("V-measure: %0.3f" % metrics.v_measure_score(labels_true, labels))
print("Adjusted Rand Index: %0.3f"
#       % metrics.adjusted_rand_score(labels_true, labels))
print("Adjusted Mutual Information: %0.3f"
#       % metrics.adjusted_mutual_info_score(labels_true, labels,
                                           average_method='arithmetic'))
print("Silhouette Coefficient: %0.3f"
#       % metrics.silhouette_score(Xtr_s, labels))

# Define clustering algorithms

algorithms = [
    KMeans(n_clusters=n_clusters_s),
    AgglomerativeClustering(n_clusters=n_clusters_s),
    DBSCAN(eps=0.3, min_samples=10),
    SpectralClustering(n_clusters=n_clusters_s),
    Birch(n_clusters=n_clusters_s),
    MiniBatchKMeans(n_clusters=n_clusters_s),
    MeanShift(),
    AffinityPropagation()
]

# Compare clustering algorithms
fig, axes = plt.subplots(2, 4, figsize=(15, 8))
fig.suptitle('Comparison of Clustering Algorithms')

for ax, algorithm in zip(axes.flatten(), algorithms):
    # Fit the model
    if algorithm.__class__.__name__ == 'MeanShift' or algorithm.__class__.__name__ == 'AffinityPropagation':
        labels = algorithm.fit_predict(Xtr_s)
    else:
        labels = algorithm.fit_predict(Xtr_s)

    # Plot the results
    ax.scatter(Xtr_s[:, 0], Xtr_s[:, 1], c=labels, cmap='viridis', edgecolors='k')
    ax.set_title(algorithm.__class__.__name__)
    ax.axis('off')

plt.show()

"""The above results show that
DBSCAN algorithm is more accurate. Also it deleted some data and considered it an outlier. Next comes MeanShift

SpectralClustering and AgglomerativeClustering are close.
The results of KMeans and SpectralClustering are close

Birch and MiniBatchKMeans almost are close.
"""